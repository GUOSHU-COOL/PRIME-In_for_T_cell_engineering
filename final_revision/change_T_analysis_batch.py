# -*- coding: utf-8 -*-
#!usr/bin/python3


import sys
from Bio import SeqIO
import numpy as np
import matplotlib.pyplot as plt
import holoviews as hv
from holoviews import opts, dim
import os
import pysam
import pandas as pd
import re
from common_analysis_utils import (
    filter_and_reverse_sam,
    flash_stitch,
    blastn,
    filter_fastq_by_length,
    get_sample_info_from_excel,
    group_genome_mapping
)
def get_edit_count(bam, chr_seq, consolidated_fq_tri_filtered_file, output_dir, sample, db,strand):
    # åˆå§‹åŒ–å­—å…¸
    sm_main_chr_dic = {}  # å­˜å‚¨ä¸»æŸ“è‰²ä½“ä¸Šçš„SMç±»å‹æ¯”å¯¹
    ms_dic = {}  # å­˜å‚¨MSç±»å‹çš„æ¯”å¯¹
    sm_ki_dic = {}
    sm_translocation_dic = {}
    small_sm_deletion = {}
    medium_sm_deletion = {}
    large_sm_deletion = {}
    small_indels_dic = {}
    wt_dic = {}
    small_ms_deletion = {}
    medium_ms_deletion = {}
    large_ms_deletion = {}
    ms_ki_dic = {}
    ms_translocation_dic = {}
    other_dic = {}
    ms_other_dict = {}
    ms_translocation_type = {}
    sm_translocation_type = {}
    deletion_type_length = {}

    # å­˜å‚¨åºåˆ—çš„å­—å…¸
    raw_fa_dic = {}
    print("consolidated_fq_tri_filtered_file",consolidated_fq_tri_filtered_file)
    # è¯»å– FASTQ æ–‡ä»¶
    with open(consolidated_fq_tri_filtered_file, "r") as raw_fq:
        for record in SeqIO.parse(raw_fq, "fastq"):
            if record.seq is None:
                print(f"emptyåºåˆ—: {record.id}")
            else:
                raw_fa_dic[record.id] = str(record.seq)  # è®°å½• ID å’Œåºåˆ—

    # è¯»å–BAMæ–‡ä»¶
    bam_file = pysam.AlignmentFile(bam, 'rb', check_sq=False)
    for read in bam_file:
        if read.qname in raw_fa_dic:
            cigar = str(read.cigarstring)
            letter = re.findall('\D', cigar)
            number = re.findall('\d+', cigar)
            number = list(map(int, number))  # å°†æ•°å­—éƒ¨åˆ†è½¬æ¢ä¸ºæ•´æ•°

            # è¿‡æ»¤åŒ…å«Hçš„CIGARå­—ç¬¦ä¸²
            if 'H' in letter:
                print("H in CIGAR")
                continue


            condition2 = read.query_sequence.count('N') <= len(read.query_sequence) * 0.05  # Nçš„æ•°é‡ä¸è¶…è¿‡5%

            if  condition2:
                # åˆ¤æ–­CIGARå­—ç¬¦ä¸²ç±»å‹
                if 'S' in letter and 'M' in letter:
                    if letter.count('S') == 1 and letter.count('M') == 1:
                        if letter.index('S') < letter.index('M'):
                            # æå–Séƒ¨åˆ†çš„åºåˆ—
                            s_length = number[letter.index('S')]
                            s_sequence = read.query_sequence[:s_length]
                            match_start_index = read.blocks[0][0] # è·å–samåºåˆ—ä¸­matchåºåˆ—çš„èµ·å§‹ # è¿™ä¸ªä½ç‚¹åŠ ä¸€æ‰æ˜¯åŒ¹é…ä¸Šçš„ è¿™ä¸ªèµ·å§‹æ˜¯0
                            match_end_index = read.blocks[0][1]

                            # è·å–Méƒ¨åˆ†çš„æ¯”å¯¹æŸ“è‰²ä½“
                            ref_name = read.reference_name  # æ¯”å¯¹æŸ“è‰²ä½“åç§°

                            # åˆ¤æ–­Méƒ¨åˆ†çš„æ¯”å¯¹æŸ“è‰²ä½“
                            if ref_name == "chr_transgene":
                                # å¦‚æœæ˜¯transgeneï¼Œå½’ç±»åˆ°KI
                                #sm_ki_dic[read.qname] = [read.query_sequence, match_start_index, match_end_index, cigar,ref_name]
                                sm_ki_dic[read.qname] = [read.query_sequence, cigar] # for unify
                            elif ref_name != chr_seq:
                                # å¦‚æœæ˜¯å…¶ä»–æŸ“è‰²ä½“ï¼Œå½’ç±»åˆ°Translocation
                                #sm_translocation_dic[read.qname] = [read.query_sequence, match_start_index, match_end_index, cigar,ref_name]
                                sm_translocation_dic[read.qname] = [read.query_sequence, cigar]
                                if strand =="+":
                                    sm_translocation_type[read.qname]={"ref_name" : ref_name,
                                                                       "chr_seq" : chr_seq,
                                                                        "reference_start":read.reference_start,
                                                                       "reference_end":read.reference_end,
                                                                       } # both strand + - read.reference_start<reference_end
                                else:
                                    sm_translocation_type[read.qname] = {"ref_name": ref_name,
                                                                        "chr_seq": chr_seq,
                                                                        "reference_start": read.reference_end,
                                                                         "reference_end": read.reference_end,
                                                                        }
                            else:
                                # å¦‚æœæ˜¯ä¸»æŸ“è‰²ä½“ï¼Œå½’ç±»åˆ°SM
                                #sm_main_chr_dic[read.qname] = [read.query_sequence, cigar]
                                sm_main_chr_dic[read.qname] = [read.query_sequence, s_sequence, match_start_index,match_end_index, cigar]
                        else:
                            # å¤„ç†MSç±»å‹
                            s_length = number[letter.index('S')]
                            s_sequence = read.query_sequence[-s_length:]
                            match_start_index = read.blocks[0][0]
                            match_end_index = read.blocks[0][1]
                            ms_dic[read.qname] = [read.query_sequence,s_sequence, match_start_index, match_end_index, cigar]
                            #ms_dic[read.qname] = [read.query_sequence,cigar]
                elif 'M' in letter and ('D' in letter or 'I' in letter):
                    # ä¸¥æ ¼åˆ¤æ–­æ˜¯å¦ä¸º M...M å½¢å¼
                    if len(letter) >= 3 and letter[0] == 'M' and letter[-1] == 'M':
                        # åªè¦ä¸­é—´åŒ…å« D æˆ– Iï¼Œå°±å½’å…¥ small_indels_dic
                        if any(mid in letter[1:-1] for mid in ['D', 'I', 'M']):
                            small_indels_dic[read.qname] = [read.query_sequence, cigar]
                            match = re.findall(r'(\d+)D', cigar)
                            deletion_length = sum(map(int, match))
                            # å­˜å‚¨åˆ°å­—å…¸
                            deletion_type_length[read.qname] = {
                                "deletion_type": "small_indels_dic",  # small_indels_dic
                                "deletion_length": deletion_length
                            }
                        else:
                            other_dic[read.qname] = [read.query_sequence, cigar]
                    else:
                        other_dic[read.qname] = [read.query_sequence, cigar]
                elif letter == ['M']:
                    wt_dic[read.qname] = [read.query_sequence, cigar]
                else:
                    other_dic[read.qname] = [read.query_sequence, cigar]
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    print('SMç±»å‹æ¯”å¯¹æ•°é‡:', len(sm_main_chr_dic))
    print('KIç±»å‹æ¯”å¯¹æ•°é‡:', len(sm_ki_dic))
    print('Translocationç±»å‹æ¯”å¯¹æ•°é‡:', len(sm_translocation_dic))
    print('Small Indelsç±»å‹æ¯”å¯¹æ•°é‡:', len(small_indels_dic))
    print('WTç±»å‹æ¯”å¯¹æ•°é‡:', len(wt_dic))
    print('MSç±»å‹æ¯”å¯¹æ•°é‡:', len(ms_dic))
    print('Otherç±»å‹æ¯”å¯¹æ•°é‡:', len(other_dic))

    # å°†SMç±»å‹çš„Séƒ¨åˆ†åºåˆ—å†™å…¥FASTAæ–‡ä»¶
    if len(sm_main_chr_dic)!=0:
        sm_main_chr_s_fa = os.path.join(output_dir, 'blastn', f'{sample}.aln_filtered_primeradd5bp_noadapter_consolidated_tri_SM_S.fa')
        os.makedirs(os.path.join(output_dir, 'blastn'), exist_ok=True) # create first
        with open(sm_main_chr_s_fa, 'w') as f:
            for key in sm_main_chr_dic.keys():
                s_seq = sm_main_chr_dic[key][1]
                f.write(f'>{key}\n{s_seq}\n')
        sm_main_chr_s_fa_output= os.path.join(output_dir, 'blastn', f'{sample}.aln_filtered_noadapter_consolidated_tri_SM_S_fa_to_hg38fa_blastn_result.txt')
        blastn(sm_main_chr_s_fa, db, sm_main_chr_s_fa_output)
        sm_main_chr_best_alignment_dict=extract_best_alignment_qstart(sm_main_chr_s_fa_output)
        small_sm_deletion, medium_sm_deletion, large_sm_deletion=categorize_sm(sm_main_chr_best_alignment_dict, sm_main_chr_dic,deletion_type_length,strand)
        # è°ƒç”¨blastnè¿›è¡Œæ¯”å¯¹ï¼ˆä»…å¯¹ä¸»æŸ“è‰²ä½“ä¸Šçš„SMç±»å‹ï¼‰
    # å°†MSç±»å‹çš„Séƒ¨åˆ†åºåˆ—å†™å…¥FASTAæ–‡ä»¶
    os.makedirs(os.path.join(output_dir, 'blastn'), exist_ok=True)

    # å°†MSç±»å‹çš„Séƒ¨åˆ†åºåˆ—å†™å…¥FASTAæ–‡ä»¶
    if len(ms_dic)!=0:
        ms_s_fa = os.path.join(output_dir, 'blastn', f'{sample}.aln_filtered_primeradd5bp_noadapter_consolidated_tri_MS_S.fa')
        with open(ms_s_fa, 'w') as f:
            for key in ms_dic.keys():
                s_seq = ms_dic[key][1]
                f.write(f'>{key}\n{s_seq}\n')

        # å®šä¹‰blastnè¾“å‡ºæ–‡ä»¶
        ms_s_fa_output = os.path.join(output_dir, 'blastn', f'{sample}.aln_filtered_noadapter_consolidated_tri_MS_S_fa_to_hg38fa_blastn_result.txt')

        # è°ƒç”¨blastnè¿›è¡Œæ¯”å¯¹ï¼ˆä»…å¯¹ä¸»æŸ“è‰²ä½“ä¸Šçš„MSç±»å‹ï¼‰
        blastn(ms_s_fa, db, ms_s_fa_output)

        # æå–blastnæ¯”å¯¹ç»“æœä¸­æœ€ä½³å¯¹é½ä¿¡æ¯
        ms_main_chr_best_alignment_dict, ms_other_dict = extract_best_alignment_qend(ms_s_fa_output,ms_dic)

        # åˆ†ç±»åˆ é™¤ç±»å‹ï¼ˆæ ¹æ®æ¯”å¯¹çš„ç»“æœè¿›è¡Œåˆ†ç±»ï¼‰
        #ms_dic[read.qname] = [read.query_sequence,s_sequence, match_start_index, match_end_index, cigar]
        small_ms_deletion, medium_ms_deletion, large_ms_deletion, ms_translocation_dic,ms_ki_dic,ms_translocation_type= (
            categorize_ms(ms_main_chr_best_alignment_dict, ms_dic,chr_seq,deletion_type_length,strand))

    return sm_ki_dic, sm_translocation_dic,small_sm_deletion, medium_sm_deletion, large_sm_deletion,small_indels_dic, wt_dic,small_ms_deletion, medium_ms_deletion, large_ms_deletion, ms_ki_dic, ms_translocation_dic, other_dic,ms_other_dict,ms_translocation_type,sm_translocation_type,deletion_type_length

def extract_best_alignment_qstart(blastn_result):
    """
    æå–BLASTNæ¯”å¯¹ç»“æœä¸­æ¯ä¸ªæŸ¥è¯¢åºåˆ—çš„æœ€ä½³æ¯”å¯¹ï¼ˆæ ¹æ® qstart == 0ï¼‰ã€‚

    å‚æ•°ï¼š
    - blastn_result: BLASTN æ¯”å¯¹ç»“æœæ–‡ä»¶è·¯å¾„

    è¿”å›ï¼š
    - best_alignment_dict: å­˜å‚¨æ¯ä¸ªæŸ¥è¯¢åºåˆ—çš„æœ€ä½³æ¯”å¯¹ä¿¡æ¯
    """
    # è¯»å–BLASTNæ¯”å¯¹ç»“æœ
    try:
        df = pd.read_csv(blastn_result, delimiter='\t', header=None)
        df.columns = ['qseqid', 'sseqid', 'pident', 'length', 'mismatch', 'gapopen',
                      'qstart', 'qend', 'sstart', 'send', 'evalue', 'bitscore']

        # æŒ‰ qseqid åˆ†ç»„
        grouped = df.groupby('qseqid')

        # å­˜å‚¨ç»“æœçš„å­—å…¸
        best_alignment_dict = {}

        # éå†æ¯ç»„ï¼Œæå– qstart == 0 çš„è¡Œ
        for name, group in grouped:
            group = group[group['qstart'] == 1]  # ä»…ä¿ç•™ qstart == 1 çš„è¡Œ

            if not group.empty:
                # æŒ‰ç…§ åºåˆ—æ¯”å¯¹ç™¾åˆ†æ¯”æœ€å¤§å€¼çš„è¡Œ
                max_row = group.loc[group['pident'].idxmax()]

                # è§£ææ‰€éœ€å­—æ®µ
                qseqid = max_row['qseqid']  # ä½œä¸º key
                s_sseqid = max_row['sseqid']
                s_sstart = int(max_row['sstart'])
                s_send = int(max_row['send'])

                # å­˜å…¥å­—å…¸
                best_alignment_dict[qseqid] = {
                    'sseqid': s_sseqid,
                    'sstart': s_sstart,
                    'send': s_send
                }

        return best_alignment_dict
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶ {blastn_result} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return {}

def extract_best_alignment_qend(blastn_result,ms_dic):
    """
    æå–BLASTNæ¯”å¯¹ç»“æœä¸­æ¯ä¸ªæŸ¥è¯¢åºåˆ—çš„æœ€ä½³æ¯”å¯¹ï¼ˆæ ¹æ® qend == s_lengthï¼‰ã€‚
    å¦‚æœæ²¡æœ‰åŒ¹é…ä¸Š qend == s_length çš„æ¯”å¯¹ï¼Œåˆ™å°†å…¶æ”¾å…¥ 'other' å­—å…¸ä¸­ã€‚

    å‚æ•°ï¼š
    - blastn_result: BLASTN æ¯”å¯¹ç»“æœæ–‡ä»¶è·¯å¾„
    - s_length: ç”¨äºæ¯”è¾ƒçš„åºåˆ—é•¿åº¦ï¼Œç­›é€‰ qend == s_length çš„æ¯”å¯¹

    è¿”å›ï¼š
    - best_alignment_dict: å­˜å‚¨æ¯ä¸ªæŸ¥è¯¢åºåˆ—çš„æœ€ä½³æ¯”å¯¹ä¿¡æ¯
    - other_dict: å­˜å‚¨æœªåŒ¹é… qend == s_length çš„æŸ¥è¯¢åºåˆ—
    """
    # è¯»å–BLASTNæ¯”å¯¹ç»“æœ
    try:
        df = pd.read_csv(blastn_result, delimiter='\t', header=None)
        df.columns = ['qseqid', 'sseqid', 'pident', 'length', 'mismatch', 'gapopen',
                      'qstart', 'qend', 'sstart', 'send', 'evalue', 'bitscore']

        # æŒ‰ qseqid åˆ†ç»„
        grouped = df.groupby('qseqid')

        # å­˜å‚¨ç»“æœçš„å­—å…¸
        best_alignment_dict = {}
        other_dict = {}

        # éå†æ¯ç»„ï¼Œæå– qend == s_length çš„è¡Œ
        for name, group in grouped:
            group_id=group['qseqid'].iloc[0]
            s_length=len(ms_dic[group_id][1])
            origin_seq=ms_dic[group_id][0]
            cigar=ms_dic[group_id][4]
            qseqid = group['qseqid'].iloc[0]
            group = group[group['qend'] == s_length]  # æ‰€æœ‰qseqidéƒ½æ˜¯ä¸€ä¸ªæ ·

            if not group.empty:
                # æŒ‰ç…§ åºåˆ—æ¯”å¯¹ç™¾åˆ†æ¯” æœ€å¤§å€¼çš„è¡Œ
                max_row = group.loc[group['pident'].idxmax()]

                # è§£ææ‰€éœ€å­—æ®µ
                qseqid = max_row['qseqid']  # ä½œä¸º key
                s_sseqid = max_row['sseqid']
                s_sstart = int(max_row['sstart'])
                s_send = int(max_row['send'])

                # å­˜å…¥å­—å…¸
                best_alignment_dict[qseqid] = {
                    'sseqid': s_sseqid,
                    'sstart': s_sstart,
                    'send': s_send
                }
            else:
                # å¦‚æœæ²¡æœ‰åŒ¹é… qend == s_lengthï¼Œåˆ™æ”¾å…¥ 'other' å­—å…¸
                other_dict[qseqid] = [origin_seq,cigar]

        return best_alignment_dict, other_dict
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶ {blastn_result} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return {}, {}

def categorize_sm(sm_main_chr_best_alignment_dict, sm_main_chr_dic,deletion_type_length,strand):
    # sm_main_chr_best_alignment_dict blast seq
    small_deletion = {}
    medium_deletion = {}
    large_deletion = {}

    for key in sm_main_chr_best_alignment_dict:
        if key in sm_main_chr_dic:
            # è®¡ç®—ç¼ºå¤±å¤§å°
            match_start = sm_main_chr_dic[key][2] #both strand+ strand- and flag 0&16 start<end
            match_end = sm_main_chr_dic[key][3]
            start = sm_main_chr_best_alignment_dict[key]["sstart"] # -strand start>send
            send= sm_main_chr_best_alignment_dict[key]["send"]
            if strand == "-":
                deletion_size = abs(send - match_end)
            else:
                deletion_size = abs(match_start - send)
            # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„åæ ‡å¯¹
            # sm_main_chr_dic[read.qname] = [read.query_sequence, cigar]
            #sm_main_chr_dic[read.qname] = [read.query_sequence, s_sequence, match_start_index, match_end_index, cigar]
            # æ„é€ å€¼åˆ—è¡¨
            value_list = [sm_main_chr_dic[key][0], sm_main_chr_dic[key][4]]

            # åˆ†ç±»å­˜å…¥ç›¸åº”å­—å…¸
            if 0 <= deletion_size < 200:
                small_deletion[key] = value_list
                deletion_type_length[key] = {
                    "deletion_type": "sm_small_indels",  # small_indels_dic
                    "deletion_length": deletion_size
                }
            elif 200 <= deletion_size < 400:
                medium_deletion[key] = value_list
                deletion_type_length[key] = {
                    "deletion_type": "sm_medium_indels",  # medium_indels_dic
                    "deletion_length": deletion_size
                }
            elif deletion_size >= 400:
                large_deletion[key] = value_list
                deletion_type_length[key] = {
                    "deletion_type": "sm_large_indels",  # large_indels_dic
                    "deletion_length": deletion_size
                }
            else:
                print(deletion_size)
                deletion_type_length[key] = {
                    "deletion_type": 'sm_other',  # other_dic
                    "deletion_length": deletion_size
                }
                # ä½¿ç”¨loggingè®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
                #logging.debug(f"Key: {key}, Deletion size: {deletion_size}, Values: {value_list}")

    return small_deletion, medium_deletion, large_deletion


def categorize_ms(ms_main_chr_best_alignment_dict, ms_main_chr_dic,main_chr,deletion_type_length,strand):
    # ms_main_chr_best_alignment_dict blast seq
    # ms_dic[read.qname] = [read.query_sequence,s_sequence, match_start_index, match_end_index, cigar]
    small_deletion = {}
    medium_deletion = {}
    large_deletion = {}
    translocation = {}
    ki_events = {}
    ms_translocation_type={}
    for key, alignment_info in ms_main_chr_best_alignment_dict.items():
        if key in ms_main_chr_dic:
            sseqid = alignment_info["sseqid"]  # è·å–æ¯”å¯¹åˆ°çš„æŸ“è‰²ä½“ç¼–å·
            sstart = alignment_info["sstart"]
            send = alignment_info["send"]
            # æ„é€ å€¼åˆ—è¡¨
            value_list = [ms_main_chr_dic[key][0], ms_main_chr_dic[key][4]]

            if sseqid == main_chr:
                # match_end- sstart
                # è®¡ç®—ç¼ºå¤±å¤§å°
                match_start = ms_main_chr_dic[key][2]
                match_end = ms_main_chr_dic[key][3]
                sstart = ms_main_chr_best_alignment_dict[key]["sstart"]
                send = ms_main_chr_best_alignment_dict[key]["send"]
                if strand == "-":
                    deletion_size = abs(match_start - sstart)
                else:
                    deletion_size = abs(sstart - match_end)
                if 0 <= deletion_size < 200:
                    small_deletion[key] = value_list
                    deletion_type_length[key] = {
                        "deletion_type": "ms_small_indels",  # small_indels_dic
                        "deletion_length": deletion_size
                    }
                elif 200 <= deletion_size < 400:
                    medium_deletion[key] = value_list
                    deletion_type_length[key] = {
                        "deletion_type": "ms_medium_indels",  # small_medium_dic
                        "deletion_length": deletion_size
                    }
                elif deletion_size >= 400:
                    large_deletion[key] = value_list
                    deletion_type_length[key] = {
                        "deletion_type": "ms_large_indels",  # small_large_dic
                        "deletion_length": deletion_size
                    }
                else:
                    deletion_type_length[key] = {
                        "deletion_type": "ms_other",  # other_dic
                        "deletion_length": deletion_size
                    }
                    print(f"Unexpected deletion size: {deletion_size}")

            elif sseqid == "chr_transgene":
                ki_events[key] = value_list  # è®¤ä¸ºæ˜¯ KI äº‹ä»¶

            else:
                ms_translocation_type[key] = {"ref_name": sseqid,
                                                     "chr_seq": main_chr,
                                              "reference_start": sstart,
                                              "reference_end": send
                                              }  # both strand + - read.reference_start<reference_end
                translocation[key] = value_list  # è®¤ä¸ºæ˜¯æ˜“ä½ï¼ˆtranslocationï¼‰

    return small_deletion, medium_deletion, large_deletion, translocation, ki_events,ms_translocation_type

def extract_prefix(sample_name):
    """
    ä»æ ·æœ¬åä¸­æå–ç¬¬ä¸€ä¸ªæ•°å­—ä¹‹å‰çš„éƒ¨åˆ†
    ä¾‹å¦‚ 'AH2-OT1' -> 'AH2'
    """
    match = re.match(r"^[^\d]*\d", sample_name)  # åŒ¹é…ä»å¤´å¼€å§‹åˆ°ç¬¬ä¸€ä¸ªæ•°å­—ä¹‹å‰çš„å­—ç¬¦
    if match:
        return match.group(0)
    else:
        return sample_name  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…ï¼Œè¿”å›åŸæ ·æœ¬å


def add_columns_from_dicts(excel_file, deletion_type_length_type, ms_translocation_type, sm_translocation_type,
                           output_file):
    # è¯»å– Excel æ–‡ä»¶
    df = pd.read_excel(excel_file, index_col=1)  # ä½¿ç”¨ç¬¬äºŒåˆ—ä½œä¸ºç´¢å¼•

    # å¤„ç† deletion_type_length_type å­—å…¸ï¼Œæ·»åŠ  deletion_type å’Œ deletion_length åˆ—
    df['deletion_type'] = df.index.map(lambda key: deletion_type_length_type.get(key, {}).get('deletion_type', None))
    df['deletion_length'] = df.index.map(
        lambda key: deletion_type_length_type.get(key, {}).get('deletion_length', None))

    # å¤„ç† ms_translocation_type å­—å…¸ï¼Œæ·»åŠ  translocation_chr åˆ—
    df['main_chr'] = df.index.map(lambda key: ms_translocation_type.get(key, {}).get('chr_seq', None))
    df['translocation_chr'] = df.index.map(lambda key: ms_translocation_type.get(key, {}).get('ref_name', None))
    df['reference_start'] = df.index.map(lambda key: ms_translocation_type.get(key, {}).get('reference_start', None))
    df['reference_end'] = df.index.map(lambda key: ms_translocation_type.get(key, {}).get('reference_end', None))
    # å¤„ç† sm_translocation_type å­—å…¸ï¼Œå¹¶ç¡®ä¿è½¬æ¢ä¸º Series
    df['main_chr'] = df['main_chr'].combine_first(
        pd.Series(df.index.map(lambda key: sm_translocation_type.get(key, {}).get('chr_seq', None)), index=df.index)
    )
    df['translocation_chr'] = df['translocation_chr'].combine_first(
        pd.Series(df.index.map(lambda key: sm_translocation_type.get(key, {}).get('ref_name', None)), index=df.index)
    )
    df['reference_start'] = df['reference_start'].combine_first(
        pd.Series(df.index.map(lambda key: sm_translocation_type.get(key, {}).get('reference_start', None)),
                  index=df.index)
    )
    df['reference_end'] = df['reference_end'].combine_first(
        pd.Series(df.index.map(lambda key: sm_translocation_type.get(key, {}).get('reference_end', None)),
                  index=df.index)
    )

    # ä¿å­˜ä¸ºæ–°çš„ Excel æ–‡ä»¶
    df.to_excel(output_file)
    print(f"âœ… æ•°æ®å·²ä¿å­˜è‡³ {output_file}")

def save_dicts_to_excel(output_file,output_evaluate_sample_dir, **dicts):
    """
    å°†å¤šä¸ªå­—å…¸ä¿å­˜åˆ°ä¸€ä¸ª Excel æ–‡ä»¶ä¸­ï¼Œæ¯ä¸ªå­—å…¸çš„åå­—ä½œä¸º "Category"ï¼ˆåˆ†ç±»ï¼‰ã€‚

    å‚æ•°ï¼š
    - output_file: str, è¾“å‡º Excel æ–‡ä»¶è·¯å¾„
    - **dicts: å…³é”®å­—å‚æ•°ï¼Œæ¯ä¸ªå­—å…¸çš„ key æ˜¯åºåˆ— IDï¼Œvalue æ˜¯ [sequence, cigar]
    """
    all_data = []  # å­˜å‚¨æ‰€æœ‰å­—å…¸çš„æ•°æ®
    category_counts = {}  # ç”¨äºç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ•°é‡

    # éå†å­—å…¸
    for category, data_dict in dicts.items():
        count = 0  # ç»Ÿè®¡å½“å‰ç±»åˆ«çš„æ•°é‡
        for key, values in data_dict.items():
            if len(values) >= 2:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®
                sequence, cigar = values[0], values[1]
                all_data.append([category, key, sequence, cigar])
                count += 1  # æ›´æ–°å½“å‰ç±»åˆ«çš„è®¡æ•°
        category_counts[category] = count  # ä¿å­˜ç±»åˆ«æ•°é‡

    # åˆ›å»º DataFrame
    df = pd.DataFrame(all_data, columns=["Category", "Key", "Sequence", "CIGAR"])

    # ä¿å­˜ä¸º Excel
    df.to_excel(output_file, index=False)
    print(f"âœ… æ•°æ®å·²ä¿å­˜è‡³ {output_file}")

    # æ‰“å°æ¯ä¸ªç±»åˆ«çš„æ•°é‡
    print("ğŸ“Š å„ç±»åˆ«æ•°é‡ç»Ÿè®¡ï¼š")
    for category, count in category_counts.items():
        print(f"  {category}: {count} æ¡æ•°æ®")
    # è®¡ç®—ç±»åˆ«æ€»æ•°
    total_count = sum(category_counts.values())

    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ¯”ä¾‹
    category_proportions = {category: count / total_count for category, count in category_counts.items()}

    # æ‰“å°æ¯ä¸ªç±»åˆ«çš„æ¯”ä¾‹
    print("ğŸ“Š å„ç±»åˆ«æ¯”ä¾‹ç»Ÿè®¡ï¼š")
    for category, proportion in category_proportions.items():
        print(f"  {category}: {proportion * 100:.2f}%")
    # ç»˜åˆ¶å †å æŸ±çŠ¶å›¾
    categories = list(category_proportions.keys())
    values = list(category_proportions.values())
    # æå– sample
    sample_name = os.path.basename(output_evaluate_sample_dir)
    # ç»˜åˆ¶å †å æŸ±çŠ¶å›¾
    plt.bar(categories, values, color='skyblue')

    # è®¾ç½®å›¾è¡¨æ ‡é¢˜å’Œæ ‡ç­¾
    plt.title(f"{sample_name} Proportion of Each Category in the Total Data", fontsize=16)
    plt.xlabel("Categories", fontsize=14)
    plt.ylabel("Proportion", fontsize=14)

    # å°† x è½´æ ‡ç­¾æ—‹è½¬ 45 åº¦ï¼Œé¿å…å †å 
    plt.xticks(rotation=45, ha='right', fontsize=12)  # ha='right' ä½¿æ ‡ç­¾å³å¯¹é½ï¼Œé¿å…åç§»

    # æ·»åŠ æ¯”ä¾‹æ–‡æœ¬æ ‡ç­¾
    for i, (category, proportion) in enumerate(category_proportions.items()):
        plt.text(i, proportion + 0.01, f"{proportion * 100:.2f}%", ha='center', va='bottom', fontsize=6)
    # åœ¨å³ä¸Šè§’æ·»åŠ ç±»åˆ«æ€»æ•°
    plt.text(0.95, 0.95, f"Total Count: {total_count}", ha='right', va='top', fontsize=10,
             transform=plt.gca().transAxes)

    # è®¾ç½® y è½´ä¸Šé™ä»¥æä¾›æ›´å¤šç©ºé—´
    plt.ylim(0, 1.05)
    # è°ƒæ•´å¸ƒå±€ï¼Œé¿å…æ ‡ç­¾è¢«è£å‰ª
    plt.tight_layout()
    # å¦‚æœæ ‡ç­¾ä»ç„¶è¶…å‡ºï¼Œå¢åŠ è°ƒæ•´è¾¹è·
    plt.subplots_adjust(top=0.85)
    # ä¿å­˜å›¾åƒ
    plt.savefig(os.path.join(output_evaluate_sample_dir,"category_proportions_freq.png"))
    # æ˜¾ç¤ºå›¾è¡¨
    plt.show()


def plot_deletion_length_distribution(excel_file, output_evaluate_sample_dir):
    df = pd.read_excel(excel_file)

    # è½¬æ¢ä¸ºæ•°å€¼ï¼Œå»é™¤ NaN
    deletion_lengths = pd.to_numeric(df['deletion_length'], errors='coerce').dropna()

    # è¿‡æ»¤æ‰ç‰¹åˆ«å°æˆ–æç«¯çš„å€¼ï¼Œæ¯”å¦‚å°äº1çš„å€¼
    deletion_lengths = deletion_lengths[deletion_lengths > 0]

    # å¯¹æ•°æ®å– log10 å˜æ¢ï¼Œé¿å…å¤§æ•°å€¼æ‹‰ä¼¸
    log_deletion_lengths = np.log10(deletion_lengths)

    # æå– sample
    sample_name = os.path.basename(output_evaluate_sample_dir)

    # 1. ç»˜åˆ¶å¯¹æ•° x è½´ + å¯¹æ•° y è½´ ç›´æ–¹å›¾
    plt.figure(figsize=(10, 6))
    plt.hist(log_deletion_lengths, bins=50, color='skyblue', edgecolor='black')
    plt.yscale("log")  # å¯¹æ•° y è½´
    plt.title(f"{sample_name} Deletion Length Frequency Distribution (Log X)", fontsize=16)
    plt.xlabel("Log10(Deletion Length)", fontsize=14)
    plt.ylabel("Frequency (log scale)", fontsize=14)
    plt.tight_layout()
    plt.savefig(os.path.join(output_evaluate_sample_dir, "deletion_length_distribution_freq_logx.png"))
    plt.show()
    # è¿‡æ»¤æ•°æ®ï¼šåªä¿ç•™ deletion_length â‰¤ 1000 çš„æ•°æ®ç‚¹
    filtered_deletion_lengths = deletion_lengths[deletion_lengths <= 1000]
    plt.figure(figsize=(10, 6))
    plt.hist(filtered_deletion_lengths, bins=50, color='salmon', edgecolor='black')
    plt.yscale("log")  # å¯¹æ•° y è½´
    plt.xlim([0, 1000])  # é™åˆ¶ X è½´èŒƒå›´åœ¨ 1k å†…
    plt.title(f"{sample_name} Deletion Length Frequency Distribution (â‰¤ 1k)", fontsize=16)
    plt.xlabel("Deletion Length", fontsize=14)
    plt.ylabel("Frequency (log scale)", fontsize=14)

    # è®¾ç½® X è½´åˆ»åº¦ï¼Œæ¯éš” 100 æ ‡ä¸€ä¸ª
    plt.xticks(np.arange(0, 1001, 100))

    plt.tight_layout()
    plt.savefig(os.path.join(output_evaluate_sample_dir, "deletion_length_distribution_freq_1k_logy.png"))
    plt.show()
def read_sample_primer_distance_excel(file_path):
    # è¯»å– Excel æ–‡ä»¶
    df = pd.read_excel(file_path, dtype=str)  # ä»¥å­—ç¬¦ä¸²æ ¼å¼è¯»å–ï¼Œé˜²æ­¢æ•°æ®ä¸¢å¤±

    # æ„å»ºå­—å…¸ï¼Œç¬¬ä¸€åˆ—ä¸º keyï¼Œåä¸¤åˆ—ä¸ºå­—å…¸å½¢å¼
    data_dict = {
        row["sample"]: {"primer_plus_5bp": row["primer_plus_5bp"],
                        "primer_to_cut_plus20bp": row["primer_to_cut_plus20bp"]}
        for _, row in df.iterrows()
    }

    return data_dict


def process_sample(sample, cm_dir, output_evaluate_dir,excel_file):
    # sample = 'Asn2-T2-T3'
    # cm_dir = '/data5/wangxin/20241001_wcx'  # TODO: /data5/wangxin/20241001_wcx åä¾§æ²¡æœ‰/
    # output_evaluate_dir ="/data5/wangxin/20241001_wcx/shuyu/20250307"


    sample_data=get_sample_info_from_excel(excel_file,sample)
    strand = sample_data["strand"]
    primer_to_cut_plus20bp=int(sample_data["primer_to_cut_plus20bp"])
    primer_plus_5bp_sequence=sample_data["primer_plus_5bp"].upper()
    chr_seq = sample_data["chr_seq"]
    cut_site = sample_data["cut_site"] # not use in T group
    adapter_sequence ="ACCACGCGTGCTCTACA"
##############  This part code only needs to be run once.
    ######### step1: merge R1 and R2 files to one merged fq file
    flash_stitch(cm_dir, sample)

    ######### step2: mapping using bwa (from merged fq file to bam)
    output_dir = cm_dir  + sample + '/bwa/'
    cmd = "mkdir {}".format(output_dir)
    print(cmd)
    os.system(cmd)

    ######## step3: UMI normalization
    output_dir = cm_dir  + sample + '/consolidate/'
    cmd = "mkdir {}".format(output_dir)
    print(cmd)
    os.system(cmd)
    fq_file = cm_dir  + sample + '/flash/' + sample + '.merge.fastq'
    consolidated_fq_file = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated.fq'
    cmd = '/home/wangxin/miniconda3_1/envs/PEM-Q/bin/python consolidate_batch.py {} {}  {} 14 0.7 1 2 {} {} {}'.format(
        sample, \
        fq_file, \
        consolidated_fq_file, \
        adapter_sequence, \
        primer_to_cut_plus20bp, \
        primer_plus_5bp_sequence,\
    )
    print(cmd)
    os.system(cmd)
    ######## step4: remove adapter using trimmomatic
    output_dir = cm_dir  + sample + '/consolidate/'
    consolidated_fq_file = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated.fq'
    consolidated_fq_gz_file = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated.fq.gz'
    consolidated_fq_tri_gz_file = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated_tri.fq.gz'
    adapter_sequence_fa_file = '/data5/wangxin/20241001_wcx/PEM-seq-sequences-AAVS1/adapter_sequence.fa'

    cmd = 'gzip -f {}'.format(consolidated_fq_file)
    print(cmd)
    os.system(cmd)

    cmd = 'java -jar /data2/wangxin/biosoft/Trimmomatic/Trimmomatic-0.38/trimmomatic-0.38.jar SE \
                                                                                                -phred33 \
                                                                                                {} \
                                                                                                {} \
                                                                                                ILLUMINACLIP:{}:2:12:10'.format(
        consolidated_fq_gz_file, \
        consolidated_fq_tri_gz_file, \
        adapter_sequence_fa_file)
    print(cmd)
    os.system(cmd)
    cmd = 'gunzip -f {}'.format(consolidated_fq_tri_gz_file)
    print(cmd)
    os.system(cmd)
#########filtered by sequence length
    consolidated_fq_tri_file = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated_tri.fq'
    consolidated_fq_tri_filtered_file = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated_tri_filtered.fq'
    filter_fastq_by_length(consolidated_fq_tri_file, consolidated_fq_tri_filtered_file, primer_to_cut_plus20bp)
    ####### step5: mapping from consolidated_tri_fq to ref genome (add transgene) using bwa
    output_dir = cm_dir  + sample + '/consolidate/'
    output_sam = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated_tri_change.sam'
    genome_name= group_genome_mapping[sample]
    genome_add_transgene = os.path.join("/data5/shuyu/result/modify_genome_db/", genome_name, f"{genome_name}_transgene.fa")

    cmd = 'bwa mem  {} {} -t 16 > {}'.format(
        genome_add_transgene, consolidated_fq_tri_filtered_file, output_sam)
    print(cmd)
    os.system(cmd)

    filter_and_reverse_output_sam = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated_tri_filter_and_reverse.sam'

    filter_and_reverse_sam(output_sam, filter_and_reverse_output_sam)
    # TODO: è¿™é‡Œçš„æ¯”å¯¹åŸºå› ç»„éœ€è¦æ ¹æ®ä¸åŒçš„æ ·æœ¬è¿›è¡Œè°ƒæ•´


    output_filter_and_reverse_bam = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated_tri_filter_and_reverse.bam'
    cmd = 'samtools sort {} -@ 15 -o {} -O BAM'.format(filter_and_reverse_output_sam, output_filter_and_reverse_bam)
    print(cmd)
    os.system(cmd)

    ######## step6 : from consolidated_tri_fq to fa
    output_dir = cm_dir  + sample + '/consolidate/'  # TODO: è¿™ä¸ªä½ç½®çš„è¾“å…¥ /data5/wangxin/20241001_wcx/  /cleandata/20241227/è¿™ä¸ªå°±å¾—å˜æˆcleandata/20241227/

############# Classify for different situations
    ######## step7: counts of different editing events (KI, small indels, large deletions, translocations, nojunctions, other editing)
    output_dir = cm_dir  + sample + '/consolidate/'
#######
    output_filter_and_reverse_bam = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated_tri_filter_and_reverse.bam'
    consolidated_fq_tri_filtered_file = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated_tri_filtered.fq'
##############
    ####### generate blastn db

    db= os.path.join("/data5/shuyu/result/modify_genome_db/", genome_name,f"{genome_name}_transgene_db")
    #db = '/data5/wangxin/20241001_wcx/PEM-seq-sequences-AAVS1/bwa_mask_KIplasmid_EF1A_addtransgene383bp_hg38.fa_blastndb'
    sm_ki_dic, sm_translocation_dic,small_sm_deletion, medium_sm_deletion, large_sm_deletion,small_indels_dic, wt_dic,small_ms_deletion, medium_ms_deletion, large_ms_deletion, ms_ki_dic, ms_translocation_dic, other_dic,ms_other_dict,ms_translocation_type,sm_translocation_type,deletion_type_length= get_edit_count(output_filter_and_reverse_bam, chr_seq, consolidated_fq_tri_filtered_file, output_dir, sample, db,strand)
    # è°ƒç”¨å‡½æ•°
    # è°ƒç”¨ save_dicts_to_excel å‡½æ•°å¹¶å°†æ‰€æœ‰å­—å…¸ä¼ å…¥
    # è°ƒç”¨ save_dicts_to_excel å‡½æ•°å¹¶å°†æ¯ä¸ªå­—å…¸ä¼ å…¥å¯¹åº”çš„ä½ç½®

    output_evaluate_sample_dir = os.path.join(output_evaluate_dir,sample)
    cmd = "mkdir {}".format(output_evaluate_sample_dir)
    print(cmd)
    os.system(cmd)

    save_dicts_to_excel(
        os.path.join(output_evaluate_dir,sample,"output.xlsx"),
        output_evaluate_sample_dir,
        sm_ki_dic=sm_ki_dic,
        sm_translocation_dic=sm_translocation_dic,
        small_sm_deletion=small_sm_deletion,
        medium_sm_deletion=medium_sm_deletion,
        large_sm_deletion=large_sm_deletion,
        small_indels_dic=small_indels_dic,
        wt_dic=wt_dic,
        small_ms_deletion=small_ms_deletion,
        medium_ms_deletion=medium_ms_deletion,
        large_ms_deletion=large_ms_deletion,
        ms_ki_dic=ms_ki_dic,
        ms_translocation_dic=ms_translocation_dic,
        other_dic=other_dic,
        ms_other_dict=ms_other_dict
    )
    add_columns_from_dicts(os.path.join(output_evaluate_dir,sample,"output.xlsx"), deletion_type_length, ms_translocation_type, sm_translocation_type,
                           os.path.join(output_evaluate_dir,sample,"add_output.xlsx"))
    # è°ƒç”¨å‡½æ•°å¹¶ä¼ å…¥ Excel æ–‡ä»¶è·¯å¾„
    excel_file = os.path.join(output_evaluate_dir,sample,"add_output.xlsx")
    plot_deletion_length_distribution(excel_file,output_evaluate_sample_dir)