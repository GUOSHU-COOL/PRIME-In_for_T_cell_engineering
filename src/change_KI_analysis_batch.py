# -*- coding: utf-8 -*-
#!usr/bin/python3
import regex as re2
import os
import pysam
import re
from Bio import SeqIO
import matplotlib.pyplot as plt
import pandas as pd

from common_analysis_utils import (
    filter_and_reverse_sam,
    flash_stitch,
    blastn,
    filter_fastq_by_length,
    get_sample_info_from_excel,
    group_genome_mapping
)

def get_edit_count(bam, chr_seq, consolidated_fq_tri_filtered_file, output_dir, sample, db,target_group,cut_site):
    # åˆå§‹åŒ–å­—å…¸
    KI={}
    reverseKI={}
    offtargetKI={}
    ms_dic = {}  # å­˜å‚¨MSç±»å‹çš„å­—å…¸

    # å­˜å‚¨åºåˆ—çš„å­—å…¸
    raw_fa_dic = {}
    print("consolidated_fq_tri_filtered_file",consolidated_fq_tri_filtered_file)
    # è¯»å– FASTQ æ–‡ä»¶
    with open(consolidated_fq_tri_filtered_file, "r") as raw_fq:
        for record in SeqIO.parse(raw_fq, "fastq"):
            if record.seq is None:
                print(f"emptyåºåˆ—: {record.id}")
            else:
                raw_fa_dic[record.id] = str(record.seq)  # è®°å½• ID å’Œåºåˆ—

    # è¯»å–BAMæ–‡ä»¶
    bam_file = pysam.AlignmentFile(bam, 'rb', check_sq=False)
    for read in bam_file:
        if read.qname in raw_fa_dic:
            if read.reference_name != "chr_transgene":
                continue
            if "AP" in sample:
                unwanted_sequence = "cggtggGAGCTGGACGGCGACGTAAACGGGGCACTTTTCGGGGAAATG"  # æ›¿æ¢ä¸ºä½ çš„ç›®æ ‡åºåˆ—
                max_mismatches = 1  # å…è®¸çš„æœ€å¤§é”™é…æ•°
                pattern = f"({unwanted_sequence}){{e<={max_mismatches}}}"  # å…è®¸æœ€å¤§2ä¸ªé”™é…

                if re.search(pattern, read.query_sequence, re2.IGNORECASE):
                    print("plasmid")
                    continue  # å¦‚æœæ‰¾åˆ°ç¬¦åˆè¦æ±‚çš„åŒ¹é…ï¼Œè·³è¿‡è¯¥ read
            cigar = str(read.cigarstring)
            letter = re.findall('\D', cigar)
            letters = "".join(re.findall(r'\D', cigar))
            if set(letters) != {'M', 'S'}:  # åªå…è®¸ M å’Œ S
                continue

            if not letters.startswith('M'):  # M å¿…é¡»åœ¨ S ä¹‹å‰
                continue
            number = re.findall('\d+', cigar)
            number = list(map(int, number))  # å°†æ•°å­—éƒ¨åˆ†è½¬æ¢ä¸ºæ•´æ•°

            condition2 = read.query_sequence.count('N') <= len(read.query_sequence) * 0.05  # Nçš„æ•°é‡ä¸è¶…è¿‡5%

            if condition2:
                # å¤„ç†MSç±»å‹
                s_length = number[letter.index('S')]
                s_sequence = read.query_sequence[-s_length:]
                match_start_index = read.blocks[0][0]
                match_end_index = read.blocks[0][1]
                ms_dic[read.qname] = [read.query_sequence, s_sequence, match_start_index, match_end_index, cigar]

    print('MSç±»å‹æ¯”å¯¹æ•°é‡:', len(ms_dic))
    # å°†MSç±»å‹çš„Séƒ¨åˆ†åºåˆ—å†™å…¥FASTAæ–‡ä»¶
    os.makedirs(os.path.join(output_dir, 'blastn'), exist_ok=True)
    if len(ms_dic)!=0:
        ms_s_fa = os.path.join(output_dir, 'blastn', f'{sample}.aln_filtered_primeradd5bp_noadapter_consolidated_tri_MS_S.fa')
        with open(ms_s_fa, 'w') as f:
            for key in ms_dic.keys():
                s_seq = ms_dic[key][1]
                f.write(f'>{key}\n{s_seq}\n')

        # å®šä¹‰blastnè¾“å‡ºæ–‡ä»¶
        ms_s_fa_output = os.path.join(output_dir, 'blastn', f'{sample}.aln_filtered_noadapter_consolidated_tri_MS_S_fa_to_hg38fa_blastn_result.txt')

        # è°ƒç”¨blastnè¿›è¡Œæ¯”å¯¹
        blastn(ms_s_fa,
               db, ms_s_fa_output)

        # æå–blastnæ¯”å¯¹ç»“æœä¸­æœ€ä½³å¯¹é½ä¿¡æ¯
        ms_main_chr_best_alignment_dict, ms_other_dict = extract_best_alignment_qend(ms_s_fa_output,ms_dic)

        # åˆ†ç±»åˆ é™¤ç±»å‹ï¼ˆæ ¹æ®æ¯”å¯¹çš„ç»“æœè¿›è¡Œåˆ†ç±»ï¼‰
        #ms_dic[read.qname] = [read.query_sequence,s_sequence, match_start_index, match_end_index, cigar]
        KI,reverseKI,offtargetKI= categorize_ms(ms_main_chr_best_alignment_dict, ms_dic,chr_seq,target_group,cut_site)

    return  KI,reverseKI,offtargetKI

def extract_best_alignment_qend(blastn_result,ms_dic):
    """
    æå–BLASTNæ¯”å¯¹ç»“æœä¸­æ¯ä¸ªæŸ¥è¯¢åºåˆ—çš„æœ€ä½³æ¯”å¯¹ï¼ˆæ ¹æ® qend == s_lengthï¼‰ã€‚
    å¦‚æœæ²¡æœ‰åŒ¹é…ä¸Š qend == s_length çš„æ¯”å¯¹ï¼Œåˆ™å°†å…¶æ”¾å…¥ 'other' å­—å…¸ä¸­ã€‚

    å‚æ•°ï¼š
    - blastn_result: BLASTN æ¯”å¯¹ç»“æœæ–‡ä»¶è·¯å¾„
    - s_length: ç”¨äºæ¯”è¾ƒçš„åºåˆ—é•¿åº¦ï¼Œç­›é€‰ qend == s_length çš„æ¯”å¯¹

    è¿”å›ï¼š
    - best_alignment_dict: å­˜å‚¨æ¯ä¸ªæŸ¥è¯¢åºåˆ—çš„æœ€ä½³æ¯”å¯¹ä¿¡æ¯
    - other_dict: å­˜å‚¨æœªåŒ¹é… qend == s_length çš„æŸ¥è¯¢åºåˆ—
    """
    # è¯»å–BLASTNæ¯”å¯¹ç»“æœ
    try:
        df = pd.read_csv(blastn_result, delimiter='\t', header=None)
        df.columns = ['qseqid', 'sseqid', 'pident', 'length', 'mismatch', 'gapopen',
                      'qstart', 'qend', 'sstart', 'send', 'evalue', 'bitscore']

        # æŒ‰ qseqid åˆ†ç»„
        grouped = df.groupby('qseqid')

        # å­˜å‚¨ç»“æœçš„å­—å…¸
        best_alignment_dict = {}
        other_dict = {}

        # éå†æ¯ç»„ï¼Œæå– qend == s_length çš„è¡Œ
        for name, group in grouped:
            group_id=group['qseqid'].iloc[0]
            s_length=len(ms_dic[group_id][1])
            origin_seq=ms_dic[group_id][0]
            cigar=ms_dic[group_id][4]
            qseqid = group['qseqid'].iloc[0]
            group = group[group['qend'] == s_length]  # æ‰€æœ‰qseqidéƒ½æ˜¯ä¸€ä¸ªæ ·

            if not group.empty:
                # æŒ‰ç…§ åºåˆ—æ¯”å¯¹ç™¾åˆ†æ¯” æœ€å¤§å€¼çš„è¡Œ
                max_row = group.loc[group['pident'].idxmax()]

                # è§£ææ‰€éœ€å­—æ®µ
                qseqid = max_row['qseqid']  # ä½œä¸º key
                s_sseqid = max_row['sseqid']
                s_sstart = int(max_row['sstart'])
                s_send = int(max_row['send'])

                # å­˜å…¥å­—å…¸
                best_alignment_dict[qseqid] = {
                    'sseqid': s_sseqid,
                    'sstart': s_sstart,
                    'send': s_send
                }
            else:
                # å¦‚æœæ²¡æœ‰åŒ¹é… qend == s_lengthï¼Œåˆ™æ”¾å…¥ 'other' å­—å…¸
                other_dict[qseqid] = [origin_seq,cigar]

        return best_alignment_dict, other_dict
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶ {blastn_result} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return {}, {}


def categorize_ms(ms_main_chr_best_alignment_dict, ms_main_chr_dic,main_chr,target_group,cut_site):
    # ms_main_chr_best_alignment_dict blast seq
    # ms_dic[read.qname] = [read.query_sequence,s_sequence, match_start_index, match_end_index, cigar]
    KI={}
    reverseKI={}
    offtargetKI={}

    for key, alignment_info in ms_main_chr_best_alignment_dict.items():
        if key in ms_main_chr_dic:
            sseqid = alignment_info["sseqid"]  # è·å–æ¯”å¯¹åˆ°çš„æŸ“è‰²ä½“ç¼–å·
            start = ms_main_chr_best_alignment_dict[key]["sstart"] # -strand start>send
            send= ms_main_chr_best_alignment_dict[key]["send"]
            # æ„é€ å€¼åˆ—è¡¨
            value_list = [ms_main_chr_dic[key][0], ms_main_chr_dic[key][4],sseqid,start,send] # read.query_sequence,cigar

            if sseqid == main_chr:
                if target_group == "A":
                    # sstart<send
                    if start<send: # same to + strand because reverse in
                        if abs(start-cut_site)<1000:
                            KI[key] = value_list
                        else:
                            offtargetKI[key] = value_list
                    else:
                        reverseKI[key] = value_list
                else:
                    if start>send:
                        if abs(start-cut_site)<1000:
                            KI[key] = value_list
                        else:
                            offtargetKI[key] = value_list
                    else:
                        reverseKI[key] = value_list
            else:
                offtargetKI[key] = value_list  # è®¤ä¸ºæ˜¯ KI äº‹ä»¶


    return KI, reverseKI, offtargetKI

def save_dicts_to_excel(output_file,output_evaluate_sample_dir, **dicts):
    """
    å°†å¤šä¸ªå­—å…¸ä¿å­˜åˆ°ä¸€ä¸ª Excel æ–‡ä»¶ä¸­ï¼Œæ¯ä¸ªå­—å…¸çš„åå­—ä½œä¸º "Category"ï¼ˆåˆ†ç±»ï¼‰ã€‚

    å‚æ•°ï¼š
    - output_file: str, è¾“å‡º Excel æ–‡ä»¶è·¯å¾„
    - **dicts: å…³é”®å­—å‚æ•°ï¼Œæ¯ä¸ªå­—å…¸çš„ key æ˜¯åºåˆ— IDï¼Œvalue æ˜¯ [sequence, cigar]
    """
    all_data = []  # å­˜å‚¨æ‰€æœ‰å­—å…¸çš„æ•°æ®
    category_counts = {}  # ç”¨äºç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ•°é‡

    # éå†å­—å…¸
    for category, data_dict in dicts.items():
        count = 0  # ç»Ÿè®¡å½“å‰ç±»åˆ«çš„æ•°é‡
        for key, values in data_dict.items():
            if len(values) >= 2:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®
                sequence, cigar,chr_seq,start,send = values[0], values[1],values[2],values[3],values[4]
                all_data.append([category, key, sequence, cigar,chr_seq,start,send])
                count += 1  # æ›´æ–°å½“å‰ç±»åˆ«çš„è®¡æ•°
        category_counts[category] = count  # ä¿å­˜ç±»åˆ«æ•°é‡

    # åˆ›å»º DataFrame
    df = pd.DataFrame(all_data, columns=["Category", "Key", "Sequence", "CIGAR", "chr_seq","Start", "Send"])

    # ä¿å­˜ä¸º Excel
    df.to_excel(output_file, index=False)
    print(f"âœ… æ•°æ®å·²ä¿å­˜è‡³ {output_file}")

    # æ‰“å°æ¯ä¸ªç±»åˆ«çš„æ•°é‡
    print("ğŸ“Š å„ç±»åˆ«æ•°é‡ç»Ÿè®¡ï¼š")
    for category, count in category_counts.items():
        print(f"  {category}: {count} æ¡æ•°æ®")
    # è®¡ç®—ç±»åˆ«æ€»æ•°
    total_count = sum(category_counts.values())

    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ¯”ä¾‹
    category_proportions = {category: count / total_count for category, count in category_counts.items()}

    # æ‰“å°æ¯ä¸ªç±»åˆ«çš„æ¯”ä¾‹
    print("ğŸ“Š å„ç±»åˆ«æ¯”ä¾‹ç»Ÿè®¡ï¼š")
    for category, proportion in category_proportions.items():
        print(f"  {category}: {proportion * 100:.2f}%")
    # ç»˜åˆ¶å †å æŸ±çŠ¶å›¾
    categories = list(category_proportions.keys())
    values = list(category_proportions.values())
    # æå– sample
    sample_name = os.path.basename(output_evaluate_sample_dir)
    # ç»˜åˆ¶å †å æŸ±çŠ¶å›¾
    plt.bar(categories, values, color='skyblue')

    # è®¾ç½®å›¾è¡¨æ ‡é¢˜å’Œæ ‡ç­¾
    plt.title(f"{sample_name} Proportion of Each Category in the Total Data", fontsize=16)
    plt.xlabel("Categories", fontsize=14)
    plt.ylabel("Proportion", fontsize=14)

    # å°† x è½´æ ‡ç­¾æ—‹è½¬ 45 åº¦ï¼Œé¿å…å †å 
    plt.xticks(rotation=45, ha='right', fontsize=12)  # ha='right' ä½¿æ ‡ç­¾å³å¯¹é½ï¼Œé¿å…åç§»

    # æ·»åŠ æ¯”ä¾‹æ–‡æœ¬æ ‡ç­¾
    for i, (category, proportion) in enumerate(category_proportions.items()):
        plt.text(i, proportion + 0.01, f"{proportion * 100:.2f}%", ha='center', va='bottom', fontsize=6)
    # åœ¨å³ä¸Šè§’æ·»åŠ ç±»åˆ«æ€»æ•°
    plt.text(0.95, 0.95, f"Total Count: {total_count}", ha='right', va='top', fontsize=10,
             transform=plt.gca().transAxes)

    # è®¾ç½® y è½´ä¸Šé™ä»¥æä¾›æ›´å¤šç©ºé—´
    plt.ylim(0, 1.05)
    # è°ƒæ•´å¸ƒå±€ï¼Œé¿å…æ ‡ç­¾è¢«è£å‰ª
    plt.tight_layout()
    # å¦‚æœæ ‡ç­¾ä»ç„¶è¶…å‡ºï¼Œå¢åŠ è°ƒæ•´è¾¹è·
    plt.subplots_adjust(top=0.85)
    # ä¿å­˜å›¾åƒ
    plt.savefig(os.path.join(output_evaluate_sample_dir,"category_proportions_freq.png"))
    # æ˜¾ç¤ºå›¾è¡¨
    plt.show()

def process_sample(sample, cm_dir, output_evaluate_dir,excel_file):
    # sample = 'AH1-KI'
    # cm_dir = '/data5/wangxin/20241001_wcx'  # TODO: /data5/wangxin/20241001_wcx åä¾§æ²¡æœ‰/
    # clean_data_dir= '/cleandata/20241227/'
    sample_data=get_sample_info_from_excel(excel_file,sample)
    primer_to_cut_plus20bp=int(sample_data["primer_to_cut_plus20bp"])
    primer_plus_5bp_sequence=sample_data["primer_plus_5bp"].upper()
    chr_seq = sample_data["chr_seq"]
    cut_site = int(sample_data["cut_site"])
    adapter_sequence ="ACCACGCGTGCTCTACA"
#############  This part code only needs to be run once.
    ######### step1: merge R1 and R2 files to one merged fq file
    flash_stitch(cm_dir, sample)

    ######### step2: mapping using bwa (from merged fq file to bam)
    output_dir = cm_dir  + sample + '/bwa/'
    cmd = "mkdir {}".format(output_dir)
    print(cmd)
    os.system(cmd)

    ######## step3: UMI normalization
    output_dir = cm_dir  + sample + '/consolidate/'
    cmd = "mkdir {}".format(output_dir)
    print(cmd)
    os.system(cmd)


    #fq_file = cm_dir + clean_data_dir + sample + '/flash/' + sample +".extendedFrags.fastq"
    fq_file = cm_dir  + sample + '/flash/' + sample + '.merge.fastq'
    consolidated_fq_file = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated.fq'

    cmd = '/home/wangxin/miniconda3_1/envs/PEM-Q/bin/python consolidate_batch.py {} {}  {} 14 0.7 1 2 {} {} {}'.format(
        sample, \
        fq_file, \
        consolidated_fq_file, \
        adapter_sequence, \
        primer_to_cut_plus20bp, \
        primer_plus_5bp_sequence,\
    )
    print(cmd)
    os.system(cmd)

    ######## step4: remove adapter using trimmomatic
    output_dir = cm_dir  + sample + '/consolidate/'
    consolidated_fq_file = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated.fq'
    consolidated_fq_gz_file = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated.fq.gz'
    consolidated_fq_tri_gz_file = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated_tri.fq.gz'
    adapter_sequence_fa_file = '/data5/wangxin/20241001_wcx/PEM-seq-sequences-AAVS1/adapter_sequence.fa'

    cmd = 'gzip -f {}'.format(consolidated_fq_file)
    print(cmd)
    os.system(cmd)

    cmd = 'java -jar /data2/wangxin/biosoft/Trimmomatic/Trimmomatic-0.38/trimmomatic-0.38.jar SE \
                                                                                                -phred33 \
                                                                                                {} \
                                                                                                {} \
                                                                                                ILLUMINACLIP:{}:2:12:10'.format(
        consolidated_fq_gz_file, \
        consolidated_fq_tri_gz_file, \
        adapter_sequence_fa_file)

    print(cmd)
    os.system(cmd)

    cmd = 'gunzip -f {}'.format(consolidated_fq_tri_gz_file)
    print(cmd)
    os.system(cmd)
#########filtered by sequence length
    output_dir = cm_dir  + sample + '/consolidate/'
    consolidated_fq_tri_file = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated_tri.fq'
    consolidated_fq_tri_filtered_file = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated_tri_filtered.fq'
    filter_fastq_by_length(consolidated_fq_tri_file, consolidated_fq_tri_filtered_file, primer_to_cut_plus20bp)
    ####### step5: mapping from consolidated_tri_fq to ref genome (add transgene) using bwa
    output_dir = cm_dir  + sample + '/consolidate/'


    output_sam = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated_tri_change.sam'

    sample_without_number_name = group_genome_mapping[sample]
    genome_add_transgene = os.path.join("/data5/shuyu/result/modify_genome_db/", sample_without_number_name, f"only_{sample_without_number_name}_transgene.fa")

    cmd = 'bwa mem  {} {} -t 16 > {}'.format(
        genome_add_transgene, consolidated_fq_tri_filtered_file, output_sam)
    print(cmd)
    os.system(cmd)

    filter_and_reverse_output_sam = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated_tri_filter_and_reverse.sam'

    filter_and_reverse_sam(output_sam, filter_and_reverse_output_sam)
    # TODO: è¿™é‡Œçš„æ¯”å¯¹åŸºå› ç»„éœ€è¦æ ¹æ®ä¸åŒçš„æ ·æœ¬è¿›è¡Œè°ƒæ•´


    output_filter_and_reverse_bam = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated_tri_filter_and_reverse.bam'
    cmd = 'samtools sort {} -@ 15 -o {} -O BAM'.format(filter_and_reverse_output_sam, output_filter_and_reverse_bam)
    print(cmd)
    os.system(cmd)


# ############# Classify for different situations
#     ######## step7: counts of different editing events (KI, small indels, large deletions, translocations, nojunctions, other editing)
    output_dir = cm_dir  + sample + '/consolidate/'
    ####### generate blastn db
    output_filter_and_reverse_bam = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated_tri_filter_and_reverse.bam'
    consolidated_fq_tri_filtered_file = output_dir + sample + '.aln_filtered_primeradd5bp_noadapter_consolidated_tri_filtered.fq'
    db= os.path.join("/data5/shuyu/result/modify_genome_db/", sample_without_number_name,f"{sample_without_number_name}_transgene_db")
    target_group=sample[0] # AP1-KI -> A
    KI,reverseKI,offtargetKI= get_edit_count(output_filter_and_reverse_bam, chr_seq, consolidated_fq_tri_filtered_file, output_dir, sample, db,target_group,cut_site)
    # è°ƒç”¨å‡½æ•°
    # è°ƒç”¨ save_dicts_to_excel å‡½æ•°å¹¶å°†æ‰€æœ‰å­—å…¸ä¼ å…¥
    # è°ƒç”¨ save_dicts_to_excel å‡½æ•°å¹¶å°†æ¯ä¸ªå­—å…¸ä¼ å…¥å¯¹åº”çš„ä½ç½®
    output_evaluate_sample_dir = os.path.join(output_evaluate_dir,sample)
    cmd = "mkdir {}".format(output_evaluate_sample_dir)
    print(cmd)
    os.system(cmd)

    save_dicts_to_excel(
        os.path.join(output_evaluate_dir,sample,"output.xlsx"),
        output_evaluate_sample_dir,
        KI=KI,
        reverseKI=reverseKI,
        offtargetKI=offtargetKI
    )
